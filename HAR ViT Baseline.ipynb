{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcb90972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec89236",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e117d189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(380, 128, 128, 3)\n",
      "(380, 9)\n",
      "(96, 128, 128, 3)\n",
      "(96, 9)\n"
     ]
    }
   ],
   "source": [
    "x = h5py.File('datasets/single_act.hdf5', 'r')\n",
    "x_train, x_test, y_train, y_test = [x['train_img'], x['test_img'], x['train_labels'], x['test_labels']]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d58438e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. class:  9\n"
     ]
    }
   ],
   "source": [
    "num_class = y_test.shape[-1]\n",
    "print('Num. class: ', num_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b40138",
   "metadata": {},
   "source": [
    "## Patch Embedding module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7507e527",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Split image into patches and then embed them.\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_size : int\n",
    "        Size of the image (it is a square).\n",
    "    patch_size : int\n",
    "        Size of the patch (it is a square).\n",
    "    in_chans : int\n",
    "        Number of input channels.\n",
    "    embed_dim : int\n",
    "        The emmbedding dimension.\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_patches : int\n",
    "        Number of patches inside of our image.\n",
    "    proj : nn.Conv2d\n",
    "        Convolutional layer that does both the splitting into patches\n",
    "        and their embedding.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, patch_size, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "                in_chans,\n",
    "                embed_dim,\n",
    "                kernel_size=patch_size,\n",
    "                stride=patch_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape `(n_samples, in_chans, img_size, img_size)`.\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape `(n_samples, n_patches, embed_dim)`.\n",
    "        \"\"\"\n",
    "        x = self.proj(x)  # (n_samples, embed_dim, n_patches ** 0.5, n_patches ** 0.5)\n",
    "        x = x.flatten(2)  # (n_samples, embed_dim, n_patches)\n",
    "        x = x.transpose(1, 2)  # (n_samples, n_patches, embed_dim)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8348e6f",
   "metadata": {},
   "source": [
    "## Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83756fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Attention mechanism.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        The input and out dimension of per token features.\n",
    "    n_heads : int\n",
    "        Number of attention heads.\n",
    "    qkv_bias : bool\n",
    "        If True then we include bias to the query, key and value projections.\n",
    "    attn_p : float\n",
    "        Dropout probability applied to the query, key and value tensors.\n",
    "    proj_p : float\n",
    "        Dropout probability applied to the output tensor.\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    scale : float\n",
    "        Normalizing consant for the dot product.\n",
    "    qkv : nn.Linear\n",
    "        Linear projection for the query, key and value.\n",
    "    proj : nn.Linear\n",
    "        Linear mapping that takes in the concatenated output of all attention\n",
    "        heads and maps it into a new space.\n",
    "    attn_drop, proj_drop : nn.Dropout\n",
    "        Dropout layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, n_heads=8, qkv_bias=True, attn_p=0., proj_p=0.):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape `(n_samples, n_patches + 1, dim)`.\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape `(n_samples, n_patches + 1, dim)`.\n",
    "        \"\"\"\n",
    "        n_samples, n_tokens, dim = x.shape\n",
    "\n",
    "        if dim != self.dim:\n",
    "            raise ValueError\n",
    "\n",
    "        qkv = self.qkv(x)  # (n_samples, n_patches + 1, 3 * dim)\n",
    "        qkv = qkv.reshape(\n",
    "                n_samples, n_tokens, 3, self.n_heads, self.head_dim\n",
    "        )  # (n_smaples, n_patches + 1, 3, n_heads, head_dim)\n",
    "        qkv = qkv.permute(\n",
    "                2, 0, 3, 1, 4\n",
    "        )  # (3, n_samples, n_heads, n_patches + 1, head_dim)\n",
    "\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        k_t = k.transpose(-2, -1)  # (n_samples, n_heads, head_dim, n_patches + 1)\n",
    "        dp = (\n",
    "           q @ k_t\n",
    "        ) * self.scale # (n_samples, n_heads, n_patches + 1, n_patches + 1)\n",
    "        attn = dp.softmax(dim=-1)  # (n_samples, n_heads, n_patches + 1, n_patches + 1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        weighted_avg = attn @ v  # (n_samples, n_heads, n_patches +1, head_dim)\n",
    "        weighted_avg = weighted_avg.transpose(\n",
    "                1, 2\n",
    "        )  # (n_samples, n_patches + 1, n_heads, head_dim)\n",
    "        weighted_avg = weighted_avg.flatten(2)  # (n_samples, n_patches + 1, dim)\n",
    "\n",
    "        x = self.proj(weighted_avg)  # (n_samples, n_patches + 1, dim)\n",
    "        x = self.proj_drop(x)  # (n_samples, n_patches + 1, dim)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa49a20",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fc5045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Multilayer perceptron.\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_features : int\n",
    "        Number of input features.\n",
    "    hidden_features : int\n",
    "        Number of nodes in the hidden layer.\n",
    "    out_features : int\n",
    "        Number of output features.\n",
    "    p : float\n",
    "        Dropout probability.\n",
    "    Attributes\n",
    "    ----------\n",
    "    fc : nn.Linear\n",
    "        The First linear layer.\n",
    "    act : nn.GELU\n",
    "        GELU activation function.\n",
    "    fc2 : nn.Linear\n",
    "        The second linear layer.\n",
    "    drop : nn.Dropout\n",
    "        Dropout layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features, out_features, p=0.):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape `(n_samples, n_patches + 1, in_features)`.\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape `(n_samples, n_patches +1, out_features)`\n",
    "        \"\"\"\n",
    "        x = self.fc1(\n",
    "                x\n",
    "        ) # (n_samples, n_patches + 1, hidden_features)\n",
    "        x = self.act(x)  # (n_samples, n_patches + 1, hidden_features)\n",
    "        x = self.drop(x)  # (n_samples, n_patches + 1, hidden_features)\n",
    "        x = self.fc2(x)  # (n_samples, n_patches + 1, out_features)\n",
    "        x = self.drop(x)  # (n_samples, n_patches + 1, out_features)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a46558",
   "metadata": {},
   "source": [
    "## Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "764b8319",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block.\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        Embeddinig dimension.\n",
    "    n_heads : int\n",
    "        Number of attention heads.\n",
    "    mlp_ratio : float\n",
    "        Determines the hidden dimension size of the `MLP` module with respect\n",
    "        to `dim`.\n",
    "    qkv_bias : bool\n",
    "        If True then we include bias to the query, key and value projections.\n",
    "    p, attn_p : float\n",
    "        Dropout probability.\n",
    "    Attributes\n",
    "    ----------\n",
    "    norm1, norm2 : LayerNorm\n",
    "        Layer normalization.\n",
    "    attn : Attention\n",
    "        Attention module.\n",
    "    mlp : MLP\n",
    "        MLP module.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, n_heads, mlp_ratio=4.0, qkv_bias=True, p=0., attn_p=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.attn = Attention(\n",
    "                dim,\n",
    "                n_heads=n_heads,\n",
    "                qkv_bias=qkv_bias,\n",
    "                attn_p=attn_p,\n",
    "                proj_p=p\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        hidden_features = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(\n",
    "                in_features=dim,\n",
    "                hidden_features=hidden_features,\n",
    "                out_features=dim,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape `(n_samples, n_patches + 1, dim)`.\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape `(n_samples, n_patches + 1, dim)`.\n",
    "        \"\"\"\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5590a625",
   "metadata": {},
   "source": [
    "## Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1971b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Simplified implementation of the Vision transformer.\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_size : int\n",
    "        Both height and the width of the image (it is a square).\n",
    "    patch_size : int\n",
    "        Both height and the width of the patch (it is a square).\n",
    "    in_chans : int\n",
    "        Number of input channels.\n",
    "    n_classes : int\n",
    "        Number of classes.\n",
    "    embed_dim : int\n",
    "        Dimensionality of the token/patch embeddings.\n",
    "    depth : int\n",
    "        Number of blocks.\n",
    "    n_heads : int\n",
    "        Number of attention heads.\n",
    "    mlp_ratio : float\n",
    "        Determines the hidden dimension of the `MLP` module.\n",
    "    qkv_bias : bool\n",
    "        If True then we include bias to the query, key and value projections.\n",
    "    p, attn_p : float\n",
    "        Dropout probability.\n",
    "    Attributes\n",
    "    ----------\n",
    "    patch_embed : PatchEmbed\n",
    "        Instance of `PatchEmbed` layer.\n",
    "    cls_token : nn.Parameter\n",
    "        Learnable parameter that will represent the first token in the sequence.\n",
    "        It has `embed_dim` elements.\n",
    "    pos_emb : nn.Parameter\n",
    "        Positional embedding of the cls token + all the patches.\n",
    "        It has `(n_patches + 1) * embed_dim` elements.\n",
    "    pos_drop : nn.Dropout\n",
    "        Dropout layer.\n",
    "    blocks : nn.ModuleList\n",
    "        List of `Block` modules.\n",
    "    norm : nn.LayerNorm\n",
    "        Layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            img_size=128,\n",
    "            patch_size=16,\n",
    "            in_chans=3,\n",
    "            n_classes=num_class,\n",
    "            embed_dim=256,\n",
    "            depth=4,  # 12\n",
    "            n_heads=8, # 12\n",
    "            mlp_ratio=4.,\n",
    "            qkv_bias=True,\n",
    "            p=0.,\n",
    "            attn_p=0.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "                img_size=img_size,\n",
    "                patch_size=patch_size,\n",
    "                in_chans=in_chans,\n",
    "                embed_dim=embed_dim,\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(\n",
    "                torch.zeros(1, 1 + self.patch_embed.n_patches, embed_dim)\n",
    "        )\n",
    "        self.pos_drop = nn.Dropout(p=p)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    dim=embed_dim,\n",
    "                    n_heads=n_heads,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    p=p,\n",
    "                    attn_p=attn_p,\n",
    "                )\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.head = nn.Linear(embed_dim, n_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run the forward pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape `(n_samples, in_chans, img_size, img_size)`.\n",
    "        Returns\n",
    "        -------\n",
    "        logits : torch.Tensor\n",
    "            Logits over all the classes - `(n_samples, n_classes)`.\n",
    "        \"\"\"\n",
    "        n_samples = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_token = self.cls_token.expand(\n",
    "                n_samples, -1, -1\n",
    "        )  # (n_samples, 1, embed_dim)\n",
    "        x = torch.cat((cls_token, x), dim=1)  # (n_samples, 1 + n_patches, embed_dim)\n",
    "        x = x + self.pos_embed  # (n_samples, 1 + n_patches, embed_dim)\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        cls_token_final = x[:, 0]  # just the CLS token\n",
    "        x = self.head(cls_token_final)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55249754",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "225f30df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_config = {\n",
    "        \"img_size\": x_test.shape[1],\n",
    "        \"in_chans\": x_test.shape[-1],\n",
    "        \"patch_size\": 16,\n",
    "        \"embed_dim\": x_test.shape[1]*2,\n",
    "        \"depth\": 4,  # 12\n",
    "        \"n_heads\": 8,  # 12\n",
    "        \"qkv_bias\": True,\n",
    "        \"mlp_ratio\": 4,\n",
    "}\n",
    "\n",
    "model_custom = VisionTransformer(**custom_config)\n",
    "# test\n",
    "inp = torch.rand(1, 3, 128, 128)\n",
    "res_c = model_custom(inp)\n",
    "res_c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e07b6d7",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c228d60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, model_name, epochs, batch_size):\n",
    "    \n",
    "    myloss = nn.CrossEntropyLoss()\n",
    "    train_acc, test_acc = list(), list()\n",
    "    patience = 20\n",
    "    max_test_acc = 0.\n",
    "    train_losses, test_losses = list(), list()\n",
    "    \n",
    "    n_tr_batches = x_train.shape[0] // batch_size \n",
    "    n_ts_batches = x_test.shape[0] // batch_size \n",
    "    print('n_tr_batches:', n_tr_batches)\n",
    "    print('n_ts_batches:', n_ts_batches)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        test_loss = 0\n",
    "        acc = 0\n",
    "        \n",
    "        # train\n",
    "        loc_acc = list()\n",
    "        for i in range(n_tr_batches):\n",
    "            # Local batches and labels\n",
    "            local_X, local_y = x_train[i*batch_size:(i+1)*batch_size], y_train[i*batch_size:(i+1)*batch_size]\n",
    "            data = torch.tensor(local_X, dtype=torch.float).permute(0, 3, 1, 2)\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "#             print(output.shape)\n",
    "#             print(torch.tensor(local_y, dtype=torch.int).shape)\n",
    "            loss = myloss(output.to(device), torch.tensor(np.argmax(local_y, -1), dtype=torch.long).to(device))\n",
    "#             loss = nn.CrossEntropyLoss(output, torch.tensor(local_y, dtype=torch.int))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.cpu().detach().numpy()/n_tr_batches\n",
    "            output = np.argmax(output.cpu().detach().numpy(), -1)\n",
    "            y = np.argmax(local_y, -1)\n",
    "            loc_acc.append(sum(output == y) / len(output) * 100)\n",
    "        train_acc.append(np.mean(loc_acc))\n",
    "        \n",
    "        acc = 0\n",
    "        # test\n",
    "        loc_acc = list()\n",
    "        for i in range(n_ts_batches):\n",
    "            # Local batches and labels\n",
    "            local_X, local_y = x_test[i*batch_size:(i+1)*batch_size], y_test[i*batch_size:(i+1)*batch_size]\n",
    "            data = torch.tensor(local_X, dtype=torch.float).permute(0, 3, 1, 2)\n",
    "            data = data.to(device)\n",
    "#             print(data.shape)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = myloss(output.to(device), torch.tensor(np.argmax(local_y, -1), dtype=torch.long).to(device))\n",
    "            test_loss += loss.cpu().detach().numpy()/n_ts_batches\n",
    "            output = np.argmax(output.cpu().detach().numpy(), -1)\n",
    "            y = np.argmax(local_y, -1)\n",
    "            loc_acc.append(sum(output == y) / len(output) * 100)\n",
    "        test_acc.append(np.mean(loc_acc))\n",
    "        \n",
    "        if test_acc[-1] > max_test_acc:\n",
    "            max_test_acc = test_acc[-1]\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        if epoch > 30 and epoch - np.argmax(test_acc) > patience:\n",
    "            break\n",
    "        lr_scheduler.step(test_loss)\n",
    "        print('Epoch: ', str(epoch+1)+'/'+str(epochs),'| Training acc: ', train_acc[-1], '| Testing acc: ', test_acc[-1])\n",
    "        \n",
    "#         if not prog_bar:\n",
    "#             plt.plot(train_losses, label=\"Train Loss\")\n",
    "#             plt.plot(test_losses, label=\"Validation Loss\")\n",
    "#             plt.xlabel(\"# Epoch\")\n",
    "#             plt.ylabel(\"Loss\")\n",
    "#             plt.legend(loc='upper right')\n",
    "#             plt.show()\n",
    "    return train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b931dbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n",
      "n_tr_batches: 23\n",
      "n_ts_batches: 6\n",
      "Epoch:  1/50 | Training acc:  14.402173913043478 | Testing acc:  19.791666666666668\n",
      "Epoch:  2/50 | Training acc:  27.17391304347826 | Testing acc:  26.041666666666668\n",
      "Epoch:  3/50 | Training acc:  32.880434782608695 | Testing acc:  27.083333333333332\n",
      "Epoch:  4/50 | Training acc:  35.32608695652174 | Testing acc:  37.5\n",
      "Epoch:  5/50 | Training acc:  43.47826086956522 | Testing acc:  38.541666666666664\n",
      "Epoch:  6/50 | Training acc:  44.56521739130435 | Testing acc:  42.708333333333336\n",
      "Epoch:  7/50 | Training acc:  54.07608695652174 | Testing acc:  43.75\n",
      "Epoch:  8/50 | Training acc:  67.66304347826087 | Testing acc:  42.708333333333336\n",
      "Epoch:  9/50 | Training acc:  67.3913043478261 | Testing acc:  44.791666666666664\n",
      "Epoch:  10/50 | Training acc:  76.3586956521739 | Testing acc:  48.958333333333336\n",
      "Epoch:  11/50 | Training acc:  70.92391304347827 | Testing acc:  53.125\n",
      "Epoch:  12/50 | Training acc:  77.17391304347827 | Testing acc:  47.916666666666664\n",
      "Epoch:  13/50 | Training acc:  89.1304347826087 | Testing acc:  46.875\n",
      "Epoch:  14/50 | Training acc:  90.48913043478261 | Testing acc:  44.791666666666664\n",
      "Epoch:  15/50 | Training acc:  90.76086956521739 | Testing acc:  46.875\n",
      "Epoch:  16/50 | Training acc:  88.58695652173913 | Testing acc:  51.041666666666664\n",
      "Epoch:  17/50 | Training acc:  89.1304347826087 | Testing acc:  51.041666666666664\n",
      "Epoch:  18/50 | Training acc:  93.20652173913044 | Testing acc:  50.0\n",
      "Epoch:  19/50 | Training acc:  96.19565217391305 | Testing acc:  48.958333333333336\n",
      "Epoch    20: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch:  20/50 | Training acc:  96.73913043478261 | Testing acc:  53.125\n",
      "Epoch:  21/50 | Training acc:  97.55434782608695 | Testing acc:  51.041666666666664\n",
      "Epoch:  22/50 | Training acc:  99.72826086956522 | Testing acc:  53.125\n",
      "Epoch:  23/50 | Training acc:  100.0 | Testing acc:  51.041666666666664\n",
      "Epoch:  24/50 | Training acc:  100.0 | Testing acc:  52.083333333333336\n",
      "Epoch:  25/50 | Training acc:  100.0 | Testing acc:  52.083333333333336\n",
      "Epoch:  26/50 | Training acc:  100.0 | Testing acc:  52.083333333333336\n",
      "Epoch:  27/50 | Training acc:  100.0 | Testing acc:  52.083333333333336\n",
      "Epoch:  28/50 | Training acc:  100.0 | Testing acc:  52.083333333333336\n",
      "Epoch:  29/50 | Training acc:  100.0 | Testing acc:  53.125\n",
      "Epoch:  30/50 | Training acc:  100.0 | Testing acc:  53.125\n",
      "Epoch    31: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch:  31/50 | Training acc:  100.0 | Testing acc:  53.125\n"
     ]
    }
   ],
   "source": [
    "custom_config = {\n",
    "        \"img_size\": 128,\n",
    "        \"in_chans\": 3,\n",
    "        \"patch_size\": 16,\n",
    "        \"embed_dim\": 256,\n",
    "        \"depth\": 8,\n",
    "        \"n_heads\": 8,\n",
    "        \"qkv_bias\": True,\n",
    "        \"mlp_ratio\": 4,\n",
    "}\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device', device)\n",
    "batch_size = 16\n",
    "epochs = 50\n",
    "\n",
    "model = VisionTransformer(**custom_config)\n",
    "model = model.to(device)\n",
    "model_name = 'har_baseline.pth'\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=10, factor=0.5)\n",
    "\n",
    "train_acc, test_acc = train(model, model_name, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc18815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f7a28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772a3f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
